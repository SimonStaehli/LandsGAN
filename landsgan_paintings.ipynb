{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sampling Landscape Photos by using Generative Models\n\n-------------\n\n## Motivation\n\nDa ich leidenschaftlich gerne fotografiere und sich meine Fotografie haupts√§chlich auf die Landschaftsfotografie fokussiert, bin ich sehr interessiert ein generatives Modell zu erstellen, das selber Landschaften kreieren kann.\n\nImpressionen: https://500px.com/p/visualframing?view=photos","metadata":{"tags":[]}},{"cell_type":"code","source":"#!nvidia-smi\n!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:58:51.26133Z","iopub.execute_input":"2022-04-06T06:58:51.261866Z","iopub.status.idle":"2022-04-06T06:59:00.609231Z","shell.execute_reply.started":"2022-04-06T06:58:51.261767Z","shell.execute_reply":"2022-04-06T06:59:00.608448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as F\nfrom torchvision.utils import make_grid\nfrom torchsummary import summary\nfrom torchvision import datasets\n\nimport matplotlib.pyplot as plt\nimport PIL\nimport os\nimport seaborn as sns\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport pickle\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:59:00.613055Z","iopub.execute_input":"2022-04-06T06:59:00.61327Z","iopub.status.idle":"2022-04-06T06:59:02.880989Z","shell.execute_reply.started":"2022-04-06T06:59:00.613245Z","shell.execute_reply":"2022-04-06T06:59:02.880181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:59:02.883281Z","iopub.execute_input":"2022-04-06T06:59:02.883494Z","iopub.status.idle":"2022-04-06T06:59:02.896114Z","shell.execute_reply.started":"2022-04-06T06:59:02.883467Z","shell.execute_reply":"2022-04-06T06:59:02.895314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"N_IMAGES = 24\nSEED = 11\n\nnp.random.seed(SEED)\nrandom_images = np.random.choice(os.listdir('../input/impressionistlandscapespaintings/content/drive/MyDrive/impressionist_landscapes_resized_1024/'), \n                                 size=N_IMAGES, replace=False)\nncols = 3\nnrows = N_IMAGES // ncols\nfig = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 3*nrows))\n\nfor i, img in enumerate(random_images):\n    plt.subplot(nrows, ncols, i+1)\n    image = PIL.Image.open('../input/impressionistlandscapespaintings/content/drive/MyDrive/impressionist_landscapes_resized_1024/' + img)\n    plt.imshow(image)\n    plt.title('Name: ' + img, fontsize=8)\n    plt.axis('off')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:59:02.898458Z","iopub.execute_input":"2022-04-06T06:59:02.89898Z","iopub.status.idle":"2022-04-06T06:59:07.510362Z","shell.execute_reply.started":"2022-04-06T06:59:02.89894Z","shell.execute_reply":"2022-04-06T06:59:07.509407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams[\"savefig.bbox\"] = 'tight'\n\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(figsize=(12,8), ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])   \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:59:07.511413Z","iopub.execute_input":"2022-04-06T06:59:07.511679Z","iopub.status.idle":"2022-04-06T06:59:07.519456Z","shell.execute_reply.started":"2022-04-06T06:59:07.511638Z","shell.execute_reply":"2022-04-06T06:59:07.518855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    Class defines custom Dataset as a workaround to the ImageFolder class, which \n    did not return correct amount of batch size.\n    \"\"\"\n\n    def __init__(self, root_dir: str, transform: 'Compose' = None, dataset_fraction: float = 1):\n        \"\"\"\n        \n        Params:\n        ------------------\n        root_dir: str\n            Defines the path from where all images should be imported from\n        \n        transform: torch.utils.transforms.Compose\n            Compose of different transforms applied during import of an image.\n            \n        all_images: list\n            List of all images names in the root dir.\n        \n        dataset_fraction: int = 1\n            fraction of dataset to take from for training. default = 1\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.dataset_fraction = dataset_fraction\n        self.images = self._build_image_list()\n        \n    def _build_image_list(self):\n        # Calculate fraction of data to take from dataset\n        dataset_size = int(round(len(os.listdir(self.root_dir)) * self.dataset_fraction, 0))\n        # Load all images from within root dri\n        all_images = np.array([img for img in os.listdir(self.root_dir) if '.jpg' in img])\n        \n        return np.random.permutation(all_images)[:dataset_size]\n    \n    def get_len_root(self):\n        return len(os.listdir(self.root_dir))\n        \n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        img_name = os.path.join(self.root_dir, self.images[idx])\n        image = PIL.Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n    \ndef get_dataloader(root_dir: str, transforms: 'torch.utils.Compose', \n                   batch_size: int, workers: int, dataset_fraction: float = 1):\n    \"\"\"\n    Functino returns a dataloader with given parameters\n    \n    Params:\n    ---------------\n    root_dir: str\n        Defines the path from where all images should be imported from\n        \n    transform: torch.utils.transforms.Compose\n        Compose of different transforms applied during import of an image.\n        \n    batch_size; int\n        Size of the imported batch\n        \n    woerkers: int\n        Amount of CPU workers for the loading of data into gpu.\n    \"\"\"\n    \n    custom_dataset = CustomDataset(root_dir=root_dir, transform=transforms, dataset_fraction=dataset_fraction)\n    \n    return DataLoader(dataset=custom_dataset, \n                      batch_size=batch_size, \n                      num_workers=workers, \n                      shuffle=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:59:07.521113Z","iopub.execute_input":"2022-04-06T06:59:07.521613Z","iopub.status.idle":"2022-04-06T06:59:07.538226Z","shell.execute_reply.started":"2022-04-06T06:59:07.52158Z","shell.execute_reply":"2022-04-06T06:59:07.537477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# According to Implementation https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\nclass DCGANDiscriminator(nn.Module):\n    def __init__(self):\n        super(DCGANDiscriminator, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n    \n# Generator Code\n\nclass DCGANGenerator(nn.Module):\n    def __init__(self):\n        super(DCGANGenerator, self).__init__()\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( 100, 64 * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( 64 * 4, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( 64 * 2, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:59:07.539574Z","iopub.execute_input":"2022-04-06T06:59:07.540045Z","iopub.status.idle":"2022-04-06T06:59:07.558773Z","shell.execute_reply.started":"2022-04-06T06:59:07.540011Z","shell.execute_reply":"2022-04-06T06:59:07.558015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  \n# According to Implementation https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\nclass DCGANDiscriminator128(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # input is (nc) x 64 x 64\n            nn.Conv2d(64, 64*2, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(64 * 8, 64 * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(64 * 16, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n    \n# Generator Code\n\nclass DCGANGenerator128(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( 100, 64 * 16, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(64 * 16),\n            nn.Dropout2d(p=.5, inplace=True),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(64 * 16, 64 * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.Dropout2d(p=.5, inplace=True),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( 64 * 8, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.Dropout2d(p=.5, inplace=True),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( 64 * 4, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.Dropout2d(p=.5, inplace=True),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( 64 * 2, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 1),\n            nn.Dropout2d(p=.5, inplace=True),\n            nn.ReLU(True),\n            # state size. (nc) x 64 x 64\n            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # out 3x128x128\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:44.421623Z","iopub.execute_input":"2022-04-06T08:07:44.421893Z","iopub.status.idle":"2022-04-06T08:07:44.438114Z","shell.execute_reply.started":"2022-04-06T08:07:44.421862Z","shell.execute_reply":"2022-04-06T08:07:44.436961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(DCGANDiscriminator128().to('cuda'), (3, 128, 128))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:44.991537Z","iopub.execute_input":"2022-04-06T08:07:44.992112Z","iopub.status.idle":"2022-04-06T08:07:45.112038Z","shell.execute_reply.started":"2022-04-06T08:07:44.99207Z","shell.execute_reply":"2022-04-06T08:07:45.110523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(DCGANGenerator128().to('cuda'), (100, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:50.403523Z","iopub.execute_input":"2022-04-06T08:07:50.403789Z","iopub.status.idle":"2022-04-06T08:07:50.534194Z","shell.execute_reply.started":"2022-04-06T08:07:50.403759Z","shell.execute_reply":"2022-04-06T08:07:50.532582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"# Params \nBATCH_SIZE = 32\nWORKERS = 2\nIMAGE_ROOT = '../input/impressionistlandscapespaintings/content/drive/MyDrive/impressionist_landscapes_resized_1024'","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:51.271692Z","iopub.execute_input":"2022-04-06T08:07:51.27243Z","iopub.status.idle":"2022-04-06T08:07:51.278552Z","shell.execute_reply.started":"2022-04-06T08:07:51.272388Z","shell.execute_reply":"2022-04-06T08:07:51.277597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = get_dataloader(root_dir=IMAGE_ROOT, \n                            transforms=transforms.Compose([transforms.Resize(size=64),\n                                                           transforms.ToTensor()]),\n                            batch_size=BATCH_SIZE, workers=WORKERS)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:51.901559Z","iopub.execute_input":"2022-04-06T08:07:51.901821Z","iopub.status.idle":"2022-04-06T08:07:51.917321Z","shell.execute_reply.started":"2022-04-06T08:07:51.901791Z","shell.execute_reply":"2022-04-06T08:07:51.916567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_batch = next(iter(dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:52.123361Z","iopub.execute_input":"2022-04-06T08:07:52.123621Z","iopub.status.idle":"2022-04-06T08:07:54.263899Z","shell.execute_reply.started":"2022-04-06T08:07:52.123591Z","shell.execute_reply":"2022-04-06T08:07:54.262806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show(make_grid(real_batch))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:54.266572Z","iopub.execute_input":"2022-04-06T08:07:54.267194Z","iopub.status.idle":"2022-04-06T08:07:54.730757Z","shell.execute_reply.started":"2022-04-06T08:07:54.267151Z","shell.execute_reply":"2022-04-06T08:07:54.729982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(real_batch) == BATCH_SIZE, f'{len(real_batch)}, {BATCH_SIZE}'\nprint('--- Test Passed ---')\ndel real_batch","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:54.732269Z","iopub.execute_input":"2022-04-06T08:07:54.732709Z","iopub.status.idle":"2022-04-06T08:07:54.739876Z","shell.execute_reply.started":"2022-04-06T08:07:54.732671Z","shell.execute_reply":"2022-04-06T08:07:54.738409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:54.744433Z","iopub.execute_input":"2022-04-06T08:07:54.745132Z","iopub.status.idle":"2022-04-06T08:07:54.759836Z","shell.execute_reply.started":"2022-04-06T08:07:54.745089Z","shell.execute_reply":"2022-04-06T08:07:54.759077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DCGAN - Deep Convolutional Generative Adversial Network\n","metadata":{}},{"cell_type":"code","source":"z_vec = torch.rand(BATCH_SIZE,100,1,1)\nprint('Z-Vector:', z_vec.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:54.761131Z","iopub.execute_input":"2022-04-06T08:07:54.761372Z","iopub.status.idle":"2022-04-06T08:07:54.769391Z","shell.execute_reply.started":"2022-04-06T08:07:54.761341Z","shell.execute_reply":"2022-04-06T08:07:54.768583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Generator\ngenerator = DCGANGenerator128()\nout = generator(z_vec)\nprint(out.shape)\nassert out.shape == torch.Size([BATCH_SIZE, 3, 128, 128])\nprint('--- Test Passed ---')\nshow(make_grid(out))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:54.771047Z","iopub.execute_input":"2022-04-06T08:07:54.77149Z","iopub.status.idle":"2022-04-06T08:07:56.095325Z","shell.execute_reply.started":"2022-04-06T08:07:54.77145Z","shell.execute_reply":"2022-04-06T08:07:56.091294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Discrimnator\ndiscriminator = DCGANDiscriminator128()\nsample_image = torch.rand((3, 128, 128))\nsample_image = sample_image.unsqueeze(0)\n\nout = discriminator(sample_image)\nprint(out.shape)\nassert out.shape == torch.Size([1, 1, 1, 1])\nprint('--- Test Passed ---')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:56.097039Z","iopub.execute_input":"2022-04-06T08:07:56.09745Z","iopub.status.idle":"2022-04-06T08:07:56.255816Z","shell.execute_reply.started":"2022-04-06T08:07:56.097414Z","shell.execute_reply":"2022-04-06T08:07:56.25507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"def train_network(generator, discriminator, criterion, \n                  optimizer_generator, optimizer_discriminator, \n                  n_epochs, dataloader, z_dim, inverse_transforms=None, \n                  epsilon=None, debug_run=False, show_images=True, label_smoothing = False,\n                  **kwargs):\n    \"\"\"\n    Trains GAN by using batch-wise fake and real data inputs.\n    \n    params:\n    --------------------\n    generator: torch.Network\n        Generator model to sample image from latent space.\n        \n    discriminator: torch.Network\n        Discrimantor Model to classify into real or fake.\n        \n    criterion: \n        Cost-Function used for the network optimizatio\n        \n    optimizer: torch.Optimizer\n        Optmizer for the network\n        \n    n_epochs: int\n        Defines how many times the whole dateset should be fed through the network\n        \n    dataloader: torch.Dataloader \n        Dataloader with the batched dataset of real data.\n        \n    epsilon: float\n        Stopping Criterion regarding to change in cost-function between two epochs.\n        \n    debug_run:\n        If true than only one batch will be put through network.\n        \n    returns:\n    ---------------------\n    generator:\n        Trained Torch Generator Model\n        \n    discriminator:\n        Trained Torch Discriminator Model\n        \n    losses: dict\n        dictionary of losses of all batches and Epochs.\n        \n    \"\"\"\n    print(20*'=', 'Start Training', 20*'=')\n    # Init lists to keep track of loss\n    training_loss_generator, training_loss_discriminator = [], []\n    batch_loss = {'Dreal':[], 'Dfake':[], 'Gfake':[]}\n    accuracy = {'real':[], 'fake':[], 'Gfake':[]}\n    # Accuracy func\n    calculate_accuracy = lambda y_true, y_pred: np.sum(y_true == y_pred) / y_true.shape[0]\n    \n    # Fixed noise for generating samples\n    fixed_noise_dim = kwargs.get('fixed_noise_dim') if kwargs.get('fixed_noise_dim') else dataloader.batch_size\n    fixed_noise = torch.randn(fixed_noise_dim, z_dim, 1, 1, device=device)\n    \n    dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(f'Training on: {dev}')\n    generator.to(dev), discriminator.to(dev)\n    criterion.to(dev)\n\n    generator.train(), discriminator.train()\n    overall_length = len(dataloader)\n    with tqdm(total=n_epochs*overall_length, disable=debug_run) as pbar:\n        for epoch in range(n_epochs):  # loop over the dataset multiple times\n            discriminator_running_loss, generator_running_loss = 0.0, 0.0\n            for i, data in enumerate(dataloader): \n                \n                # Get Batch of real images and pass to Discriminator which first trains on only real data.\n                real_images = data.to(dev)\n                \n                # Create Labels\n                real_labels = torch.full(size=(real_images.shape[0], ), fill_value=real_label, \n                                         dtype=torch.float, device=device)\n                fake_labels = torch.full(size=(real_images.shape[0], ), fill_value=fake_label, \n                                         dtype=torch.float, device=device)\n                flipped_fake_labels = torch.full(size=(real_images.shape[0], ), fill_value=real_label, \n                                                 dtype=torch.float, device=device)  \n                if label_smoothing:\n                    real_labels += torch.normal(mean=0, std=.1, size=(real_labels.shape[0], ), device=dev)\n                    fake_labels += torch.abs(torch.normal(mean=0, std=.1, size=(real_labels.shape[0], ), device=dev))\n                    flipped_fake_labels += torch.normal(mean=0, std=.1, size=(real_labels.shape[0], ), device=dev)\n                \n                \n                # Create noise from normal distribution\n                # Generate Batch of latent vectors\n                noise_vec = torch.randn(real_images.shape[0], Z_DIM, 1, 1, device=device)                \n                \n                # -----------------------------------------\n                # REAL IMAGES - Training Discriminator \n                # -----------------------------------------\n                discriminator.zero_grad()\n                \n                # Pass into Discriminator\n                out = discriminator(real_images).view(-1)\n                loss_real = criterion(out, real_labels)\n                loss_real.backward()\n                \n                batch_loss['Dreal'].append(loss_real.mean().item())\n                pred = torch.round(out)\n                accuracy['real'].append(calculate_accuracy(real_labels.cpu().numpy(), \n                                                           pred.detach().cpu().numpy()))  \n                 \n                # -----------------------------------------\n                # Fake Images - Discriminator\n                # -----------------------------------------\n                \n                # Pass noise to generator\n                fake_images = generator(noise_vec)\n                # Passed generated images to discriminator\n                out = discriminator(fake_images.detach()).view(-1)\n                \n                # Calculate Loss on Fake images\n                loss_fake = criterion(out, fake_labels)\n                loss_fake.backward()\n                \n                d_loss = loss_real + loss_fake\n                \n                # Udpate Discriminator Optimizer\n                optimizer_discriminator.step()\n                \n                batch_loss['Dfake'].append(loss_fake.mean().item())\n                \n                # Calculate overall loss over both and append\n                discriminator_running_loss += d_loss.mean().item()\n                \n                # Calculate accuracy on real images \n                pred = torch.round(out)\n                accuracy['fake'].append(calculate_accuracy(fake_labels.cpu().numpy(), \n                                                           pred.detach().cpu().numpy()))\n                # -----------------------------------------\n                # FAKE - Training of Generator \n                # -----------------------------------------\n                generator.zero_grad()\n                                \n                #fake_images = generator(G_z_vec)\n                out = discriminator(fake_images).view(-1)\n                loss_generator = criterion(out, flipped_fake_labels)\n                loss_generator.backward()\n                \n                # Update G\n                optimizer_generator.step()\n                \n                # Append Losses \n                batch_loss['Gfake'].append(loss_generator.mean().item())\n                pred = torch.round(out)\n                accuracy['Gfake'].append(calculate_accuracy(1-flipped_fake_labels.cpu().numpy(), \n                                                            pred.detach().cpu().numpy()))\n                generator_running_loss += loss_generator.mean().item()\n                \n                # ----------------------------------------\n                # calc and print stats\n                pbar.set_description(f'Epoch: {epoch+1}/{n_epochs} // GRL: {round(generator_running_loss, 3)}  -- '+\n                                     f'DRL: {round(discriminator_running_loss, 3)} ')\n                pbar.update(1)\n                if debug_run:\n                    print('- Training Iteration passed. -')\n                    break\n                \n            print(f'Epoch {epoch+1} // [GRL: {np.round(generator_running_loss, 3)}]  -- '+\n                  f'[DRL: {np.round(discriminator_running_loss, 3)}] -- ' + \n                  f'[Accuracy (R/F): {round(np.mean(accuracy[\"real\"][:-i]), 3)} - ',\n                  f'{round(np.mean(accuracy[\"fake\"][:-i]), 3)}]')\n            \n            training_loss_generator.append(generator_running_loss)\n            training_loss_discriminator.append(discriminator_running_loss)\n            \n            if show_images:\n                with torch.no_grad():\n                    imgs = generator(fixed_noise).detach().cpu()\n                    if inverse_transforms:\n                        imgs = inverse_transforms(imgs)\n                    show(make_grid(imgs))\n                    \n            \n            if epsilon:\n                if epoch > 0:\n                    diff = np.abs(training_loss_generator[-2] - generator_running_loss)\n                    if diff < epsilon:\n                        print('- Network Converged. Stopped Training. -')\n                        break\n\n            if debug_run:\n                # Breaks loop \n                break\n        \n    print(20*'=', 'Finished Training', 20*'=')\n    return generator, discriminator, dict(generator=training_loss_generator,\n                                          discriminator=training_loss_discriminator,\n                                          batch_loss=batch_loss, accuracy=accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:56.315353Z","iopub.execute_input":"2022-04-06T08:07:56.315654Z","iopub.status.idle":"2022-04-06T08:07:56.347494Z","shell.execute_reply.started":"2022-04-06T08:07:56.315623Z","shell.execute_reply":"2022-04-06T08:07:56.346734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:57.25165Z","iopub.execute_input":"2022-04-06T08:07:57.252277Z","iopub.status.idle":"2022-04-06T08:07:57.258307Z","shell.execute_reply.started":"2022-04-06T08:07:57.252236Z","shell.execute_reply":"2022-04-06T08:07:57.25735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Init Networks\ndiscriminator = DCGANDiscriminator128()\ndiscriminator.apply(weights_init)\n\ngenerator = DCGANGenerator128()\ngenerator.apply(weights_init)\n\ntorch.cuda.empty_cache()\n\n####################################################\n# Training Parameters\nZ_DIM = 100\nIMAGE_SIZE = 128\nN_EPOCHS = 100\nDEBUG_RUN = False\nIMAGE_ROOT = '../input/impressionistlandscapespaintings/content/drive/MyDrive/impressionist_landscapes_resized_1024'\nWORKERS = 0 \nBATCH_SIZE = 64 \nDS_FRACTION = 1 # Will only take a fraction of the dataset\nfixed_noise_dim = 32\n####################################################\n\ndataloader = get_dataloader(root_dir=IMAGE_ROOT, \n                            transforms=transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                                                           transforms.ToTensor(), \n                                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n                            batch_size=BATCH_SIZE, workers=WORKERS, dataset_fraction=DS_FRACTION)\n\n\"\"\"dataloader = get_dataloader(root_dir=IMAGE_ROOT, \n                            transforms=transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                                                           transforms.ToTensor()]),\n                            batch_size=BATCH_SIZE, workers=WORKERS, dataset_fraction=DS_FRACTION)\"\"\"\n\n# Defines label to train on \nreal_label = 1.\nfake_label = 0.\n\ncriterion = nn.BCELoss()\noptimizer_generator = optim.Adam(generator.parameters(), lr=.0002, betas=(.5, .999))\noptimizer_discriminator = optim.Adam(discriminator.parameters(), lr=.0002, betas=(.5, .999))\n\ninverse_transforms = transforms.Compose([transforms.Normalize((0, 0, 0), (1/.5, 1/.5, 1/.5)), \n                                         transforms.Normalize((-0.5, -0.5, -0.5), (1, 1, 1))])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:07:58.052582Z","iopub.execute_input":"2022-04-06T08:07:58.053408Z","iopub.status.idle":"2022-04-06T08:07:58.432141Z","shell.execute_reply.started":"2022-04-06T08:07:58.053363Z","shell.execute_reply":"2022-04-06T08:07:58.431371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator, discriminator, loss = train_network(generator=generator, discriminator=discriminator, \n                                               criterion=criterion, optimizer_generator=optimizer_generator, \n                                               optimizer_discriminator=optimizer_discriminator, z_dim=Z_DIM,\n                                               n_epochs=N_EPOCHS, dataloader=dataloader, debug_run=DEBUG_RUN, \n                                               fixed_noise_dim=fixed_noise_dim, label_smoothing=False, \n                                               inverse_transforms = inverse_transforms)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:13.215759Z","iopub.execute_input":"2022-04-06T13:25:13.216557Z","iopub.status.idle":"2022-04-06T15:03:22.850394Z","shell.execute_reply.started":"2022-04-06T13:25:13.216515Z","shell.execute_reply":"2022-04-06T15:03:22.849758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_batch_loss(loss: dict):\n    tmp = pd.DataFrame.from_dict(loss['batch_loss']) \\\n        .reset_index() \\\n        .melt(id_vars='index', value_name='loss', var_name='type')\n    \n    fig = plt.subplots(figsize=(8, 4))\n    p = sns.lineplot(x=tmp['index'], y=tmp['loss'], style=tmp['type'], \n                     hue=tmp['type'], palette='Paired')\n    p.set_title('Batch Loss', loc='left')\n    p.set_xlabel('Batch')\n    p.set_ylabel('BCE-Loss')\n    sns.despine()\n    plt.show()\n    \ndef plot_batch_acc(loss: dict):\n    tmp = pd.DataFrame.from_dict(loss['accuracy']) \\\n        .reset_index() \\\n        .melt(id_vars='index', value_name='acc', var_name='type')\n    \n    fig = plt.subplots(figsize=(8, 4))\n    p = sns.lineplot(x=tmp['index'], y=tmp['acc'], style=tmp['type'], \n                     hue=tmp['type'], palette='Paired')\n    p.set_title('Batch Accuracy', loc='left')\n    p.set_xlabel('Batch')\n    p.set_ylabel('Accuracy')\n    sns.despine()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:28:25.909763Z","iopub.execute_input":"2022-04-05T11:28:25.91039Z","iopub.status.idle":"2022-04-05T11:28:25.946514Z","shell.execute_reply.started":"2022-04-05T11:28:25.910296Z","shell.execute_reply":"2022-04-05T11:28:25.945846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch_loss(loss)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:28:25.965098Z","iopub.execute_input":"2022-04-05T11:28:25.965397Z","iopub.status.idle":"2022-04-05T11:28:26.025117Z","shell.execute_reply.started":"2022-04-05T11:28:25.965367Z","shell.execute_reply":"2022-04-05T11:28:26.023895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch_acc(loss)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:28:26.026006Z","iopub.status.idle":"2022-04-05T11:28:26.026383Z","shell.execute_reply.started":"2022-04-05T11:28:26.026197Z","shell.execute_reply":"2022-04-05T11:28:26.026218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save states\nSAVE_OUTPUT = True\n\nif SAVE_OUTPUT:\n    with open('./losses_s128_12eps.pkl', 'wb') as pkl_file:\n        pickle.dump(loss, pkl_file)\n    \n    torch.save(generator, f'./generator_{N_EPOCHS}eps_{IMAGE_SIZE}pix')\n    torch.save(discriminator, f'./discriminator_{N_EPOCHS}eps_{IMAGE_SIZE}pix')","metadata":{"execution":{"iopub.status.busy":"2022-04-05T09:36:44.215963Z","iopub.execute_input":"2022-04-05T09:36:44.216508Z","iopub.status.idle":"2022-04-05T09:36:44.303475Z","shell.execute_reply.started":"2022-04-05T09:36:44.216468Z","shell.execute_reply":"2022-04-05T09:36:44.302737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image_batch(Z):\n    \"\"\"\"\"\"\n    ncols = 6\n    nrows = Z.shape[0] // ncols + 1\n    \n    with torch.no_grad():\n        image_batch = generator(Z)\n    \n    inverse_transforms = transforms.Compose([transforms.Normalize(mean=(0, 0, 0), std=(1/0.5, 1/0.5, 1/0.5)),\n                                             transforms.Normalize(mean=(-.5, -.5, -.5), std=(1, 1, 1)),\n                                             transforms.ToPILImage()])     \n    fig = plt.subplots(figsize=(16, nrows*3))\n    \n    for i, image in enumerate(image_batch):\n        plt.subplot(nrows, ncols, i+1)\n        plt.imshow(inverse_transforms(image))\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T13:03:23.965797Z","iopub.execute_input":"2022-04-03T13:03:23.968754Z","iopub.status.idle":"2022-04-03T13:03:23.981302Z","shell.execute_reply.started":"2022-04-03T13:03:23.968707Z","shell.execute_reply":"2022-04-03T13:03:23.980378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_image_batch(Z=loss['generated_images'][-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T13:03:25.271343Z","iopub.execute_input":"2022-04-03T13:03:25.272043Z","iopub.status.idle":"2022-04-03T13:03:25.28631Z","shell.execute_reply.started":"2022-04-03T13:03:25.271991Z","shell.execute_reply":"2022-04-03T13:03:25.285441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Images","metadata":{}},{"cell_type":"markdown","source":"generator = torch.load('./models/generator_12eps')","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    out = generator(torch.randn(1, Z_DIM, 1, 1, device=device))\n    \ninverse_transforms = transforms.Compose([transforms.ToPILImage()]) \nimage = inverse_transforms(out[0].to('cpu'))\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T18:01:22.529066Z","iopub.execute_input":"2022-04-03T18:01:22.529335Z","iopub.status.idle":"2022-04-03T18:01:22.76146Z","shell.execute_reply.started":"2022-04-03T18:01:22.529306Z","shell.execute_reply":"2022-04-03T18:01:22.760835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out[0].shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"torch.save(discriminator, 'discriminator_4eps')","metadata":{}},{"cell_type":"markdown","source":"# Sources\n\nhttps://paperswithcode.com/dataset/lhq\n\nhttps://universome.github.io/alis\n\nhttps://arxiv.org/pdf/1511.06434.pdf\n\nhttps://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n\nhttps://github.com/soumith/ganhacks","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}